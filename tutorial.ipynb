{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTORIAL ON HOW TO USE H-CAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import PyTorchTrainer\n",
    "from src.dataloader import MultiFormatDataLoader\n",
    "from src.models import *\n",
    "from src.evaluator import Evaluator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define experimental parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardness types:\n",
    "# - \"uniform\": Uniform mislabeling\n",
    "# - \"asymmetric\": Asymmetric mislabeling\n",
    "# - \"adjacent\" : Adjacent mislabeling\n",
    "# - \"instance\": Instance-specific mislabeling\n",
    "# - \"ood_covariate\": Near-OOD Covariate Shift\n",
    "# - \"domain_shift\": Specific type of Near-OOD\n",
    "# - \"far_ood\": Far-OOD shift (out-of-support)\n",
    "# - \"zoom_shift\": Zoom shift  - type of Atypical for images\n",
    "# - \"crop_shift\": Crop shift  - type of Atypical for images\n",
    "\n",
    "\n",
    "\n",
    "hardness = \"uniform\"\n",
    "p=0.1\n",
    "dataset = \"mnist\"\n",
    "model_name = \"LeNet\"\n",
    "epochs = 10\n",
    "seed = 0\n",
    "\n",
    "# Defined by prior or domain knowledge\n",
    "if hardness ==\"instance\":\n",
    "    if dataset == \"mnist\":\n",
    "        rule_matrix = {\n",
    "                    1: [7],\n",
    "                    2: [7],\n",
    "                    3: [8],\n",
    "                    4: [4],\n",
    "                    5: [6],\n",
    "                    6: [5],\n",
    "                    7: [1, 2],\n",
    "                    8: [3],\n",
    "                    9: [7],\n",
    "                    0: [0]\n",
    "                }\n",
    "    if dataset == \"cifar\":\n",
    "\n",
    "        rule_matrix = {\n",
    "                    0: [2],   # airplane (unchanged)\n",
    "                    1: [9],   # automobile -> truck\n",
    "                    2: [9],   # bird (unchanged)\n",
    "                    3: [5],   # cat -> automobile\n",
    "                    4: [5,7],   # deer (unchanged)\n",
    "                    5: [3, 4],   # dog -> cat\n",
    "                    6: [6],   # frog (unchanged)\n",
    "                    7: [5],   # horse -> dog\n",
    "                    8: [7],   # ship (unchanged)\n",
    "                    9: [9],   # truck -> horse\n",
    "                }\n",
    "\n",
    "else:\n",
    "    rule_matrix = None\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define HCMs to evaluate --- if unspecified we will evaluate all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterization_methods =  [\n",
    "            \"aum\",\n",
    "            \"data_uncert\", # for both Data-IQ and Data-Maps\n",
    "            \"el2n\",\n",
    "            \"grand\",\n",
    "            \"cleanlab\",\n",
    "            \"forgetting\",\n",
    "            \"vog\",\n",
    "            \"prototypicality\",\n",
    "            \"allsh\",\n",
    "            \"loss\",\n",
    "            \"conf_agree\",\n",
    "            \"detector\"\n",
    "        ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "if dataset == 'cifar':\n",
    "    # Define transforms for the dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(size=32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Load the CIFAR-10 dataset\n",
    "    train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "elif dataset =='mnist':\n",
    "    # Define transforms for the dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "total_samples = len(train_dataset)\n",
    "num_classes = 10\n",
    "\n",
    "# Set device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Dataloader module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows importing data in multiple formats\n",
    "\n",
    "dataloader_class = MultiFormatDataLoader(data=train_dataset,\n",
    "                                        target_column=None,\n",
    "                                        data_type='torch_dataset',\n",
    "                                        data_modality='image',\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=0,\n",
    "                                        transform=None,\n",
    "                                        image_transform=None,\n",
    "                                        perturbation_method=hardness,\n",
    "                                        p=p,\n",
    "                                        rule_matrix=rule_matrix\n",
    "        )\n",
    "\n",
    "\n",
    "dataloader, dataloader_unshuffled = dataloader_class.get_dataloader()\n",
    "flag_ids = dataloader_class.get_flag_ids()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: TRAINER module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the neural network and optimizer\n",
    "if dataset == 'cifar':\n",
    "    if model_name == 'LeNet':\n",
    "        model = LeNet(num_classes=10).to(device)\n",
    "    if model_name == 'ResNet':\n",
    "        model = ResNet18().to(device)\n",
    "elif dataset == 'mnist':\n",
    "    if model_name == 'LeNet':\n",
    "        model = LeNetMNIST(num_classes=10).to(device)\n",
    "    if model_name == 'ResNet':\n",
    "        model = ResNet18MNIST().to(device)\n",
    "        \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Instantiate the PyTorchTrainer class\n",
    "trainer = PyTorchTrainer(model=model,\n",
    "                            criterion=criterion,\n",
    "                            optimizer=optimizer,\n",
    "                            lr=0.001,\n",
    "                            epochs=epochs,\n",
    "                            total_samples=total_samples,\n",
    "                            num_classes=num_classes,\n",
    "                            characterization_methods=characterization_methods,\n",
    "                            device=device)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(dataloader, dataloader_unshuffled)\n",
    "\n",
    "hardness_dict = trainer.get_hardness_methods()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Evaluator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = Evaluator(hardness_dict=hardness_dict, flag_ids=flag_ids, p=p)\n",
    "\n",
    "eval_dict, raw_scores_dict = eval.compute_results()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardness_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a7e60037fa700cd8eaefe68718883a7c5484a19cbbdd784476a6a2fe63bc7d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
