{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze & plot results from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {'allsh': \"Statistical\",\n",
    " 'aum': \"LB:Output\",\n",
    " 'cleanlab': \"Statistical\",\n",
    " 'conf_agree': \"Statistical\",\n",
    " 'dataiq':\"LB:Output\",\n",
    " 'datamaps': \"LB:Output\",\n",
    " 'detector': \"LB:Stats\",\n",
    " 'el2n': \"LB:Stats\",\n",
    " 'forgetting': \"LB:Other\",\n",
    " 'grand': \"LB:Grad\",\n",
    " 'loss': \"LB:Other\",\n",
    " \"protypicality\": \"Dist\",\n",
    " 'vog': \"LB:Grad\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots \n",
    "plt.style.reload_library()\n",
    "plt.style.use([\"science\", \"ieee\", \"no-latex\", \"notebook\", \"grid\", \"vibrant\"])\n",
    "\n",
    "# Load the WANDB YAML file\n",
    "with open('./wandb.yaml') as file:\n",
    "    wandb_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_data['wandb_key'] \n",
    "wandb_entity = wandb_data['wandb_entity'] \n",
    "\n",
    "color_mapping = {\n",
    "    'allsh': 'red',\n",
    "    'aum': 'blue',\n",
    "    'cleanlab': 'green',\n",
    "    'conf_agree': 'orange',\n",
    "    'dataiq': 'purple',\n",
    "    'datamaps': 'brown',\n",
    "    'detector': 'pink',\n",
    "    'el2n': 'gray',\n",
    "    'forgetting': 'olive',\n",
    "    'grand': 'cyan',\n",
    "    'loss': 'magenta',\n",
    "    'prototypicality': 'lime',\n",
    "    'vog': 'teal'\n",
    "}\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan', 'magenta', 'lime', 'teal']\n",
    "methods = ['allsh', 'aum', 'cleanlab', 'conf_agree', 'dataiq', 'datamaps', 'detector', 'el2n', 'forgetting', 'grand', 'loss', 'prototypicality', 'vog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_dict(dict_list):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Initialize dictionaries to store the total and count of each metric\n",
    "    totals = defaultdict(lambda: defaultdict(int))\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Iterate over the list of dictionaries\n",
    "    for d in dict_list:\n",
    "        # Iterate over each metric in the dictionary\n",
    "        for key, metrics in d.items():\n",
    "            for metric, value in metrics.items():\n",
    "                # Add the value to the total and increment the count\n",
    "                totals[key][metric] += value\n",
    "                counts[key][metric] += 1\n",
    "\n",
    "    # Compute the means\n",
    "    means = {key: {metric: total / counts[key][metric]\n",
    "                for metric, total in metrics.items()}\n",
    "            for key, metrics in totals.items()}\n",
    "    \n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_and_perf_dicts(performance_dict, prop_list=[0.1,0.2,0.3,0.4,0.5]):\n",
    "    \n",
    "    data = performance_dict\n",
    "    perf_dict= {}\n",
    "\n",
    "    for idx, p in enumerate(prop_list):\n",
    "        perf_dict[p] = {}\n",
    "        for key in data:\n",
    "            perf_dict[p][key] = data[key][idx]\n",
    "\n",
    "    # Create a new dictionary to store the ranks\n",
    "    rank_dict = {}\n",
    "    d= perf_dict\n",
    "    # Iterate over the keys of the original dictionary\n",
    "    for key in d.keys():\n",
    "        \n",
    "        # Get the values of the current key\n",
    "        values = d[key]\n",
    "        \n",
    "        # Sort the values in descending order and get the keys\n",
    "        sorted_keys = sorted(values, key=lambda x: values[x], reverse=True)\n",
    "        \n",
    "        # Create a dictionary to store the ranks\n",
    "        rank_values = {}\n",
    "        \n",
    "        # Assign ranks to the keys\n",
    "        for i, k in enumerate(sorted_keys):\n",
    "            rank_values[k] = i+1\n",
    "        \n",
    "        # Add the rank values to the rank dictionary\n",
    "        rank_dict[key] = rank_values\n",
    "\n",
    "    data = rank_dict\n",
    "\n",
    "    return data, perf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "\n",
    "# set parameters you wish to plot: for example\n",
    "models = ['LeNet', 'ResNet']\n",
    "hardness_methods = ['uniform', 'asymmetric', 'instance', 'adjacent', 'domain_shift', 'ood_covariate', 'far_ood', 'crop_shift', 'zoom_shift']\n",
    "datasets = ['mnist', 'cifar']\n",
    "# datasets = ['cover', \"eye\", \"diabetes\"]#, 'cifar']\n",
    "\n",
    "exp_combos =  list(itertools.product(models, datasets, hardness_methods))\n",
    "\n",
    "\n",
    "perf_prc_dict = {method: [] for method in hardness_methods}\n",
    "auc_prc_dict = {method: [] for method in hardness_methods}\n",
    "data_prc_dict = {method: [] for method in hardness_methods}\n",
    "\n",
    "perf_roc_dict = {method: [] for method in hardness_methods}\n",
    "auc_roc_dict = {method: [] for method in hardness_methods}\n",
    "data_roc_dict = {method: [] for method in hardness_methods}\n",
    "\n",
    "\n",
    "failed = []\n",
    "\n",
    "# if folder doesn't exist create it\n",
    "if not os.path.exists(\"results_folder\"):\n",
    "    os.makedirs(\"results_folder\")\n",
    "\n",
    "# set the base folder\n",
    "base_folder = \"results_folder\"\n",
    "\n",
    "\n",
    "# Replace 'your-username' with your wandb username\n",
    "username = wandb_entity\n",
    "\n",
    "for exp_combo in tqdm(exp_combos):\n",
    "\n",
    "    try:\n",
    "        model = exp_combo[0]\n",
    "        dataset_name = exp_combo[1]\n",
    "        hardness = exp_combo[2]\n",
    "\n",
    "        folder = f\"{base_folder}/{hardness}_{dataset_name}_{model}\"\n",
    "\n",
    "        myproject = f\"{hardness}_{dataset_name}_{model}\"\n",
    "\n",
    "        # check if folder does not exist in create folder then create and sub folders: cd_plots, heatmaps, violin, mat_compare\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        prop_list = [0.1,0.2,0.3,0.4,0.5]\n",
    "        if hardness == 'crop_shift' or hardness == 'zoom_shift' or hardness == 'far_ood':\n",
    "            prop_list = [0.05,0.1,0.15,0.2,0.25]\n",
    "        api = wandb.Api()\n",
    "        runs = api.runs(f\"{username}/{myproject}\")\n",
    "        \n",
    "        # Collect the metrics from all runs into a list\n",
    "        all_metrics = []\n",
    "        for run in runs:\n",
    "            metrics = run.history()\n",
    "            metrics[\"run_id\"] = run.id\n",
    "            all_metrics.append(metrics)\n",
    "\n",
    "        # Combine the metrics into a single DataFrame\n",
    "        df = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "        # Group the DataFrame by 'run_id'\n",
    "        grouped = df.groupby('run_id')\n",
    "\n",
    "        # Merge rows with the same 'run_id' by taking the mean of each group\n",
    "        grouped_df = grouped.first().reset_index()\n",
    "\n",
    "        merged_df = grouped_df[grouped_df['hardness'] == hardness]\n",
    "\n",
    "        # Extract the columns with 'accuracy' in their name\n",
    "        merged_df = merged_df.sort_values('p')\n",
    "\n",
    "        # Define the columns for each subplot\n",
    "        accuracy_cols = np.sort([col for col in merged_df.columns if 'accuracy' in col])\n",
    "        auprc_cols = np.sort([col for col in merged_df.columns if 'auc_prc' in col and 'accuracy' not in col])\n",
    "        auroc_cols = np.sort([col for col in merged_df.columns if 'auc_roc' in col and 'accuracy' not in col])\n",
    "\n",
    "        # Plot the recall columns on the second subplot\n",
    "        auprc_dict = {}\n",
    "        for col in auprc_cols:\n",
    "            matching_methods = [method for method in methods if method in col]\n",
    "            color = [color_mapping[method] for method in matching_methods][0]\n",
    "            p_list = merged_df['p'].unique()\n",
    "            auprc_list = []\n",
    "            for p in p_list:\n",
    "                score = np.mean(merged_df[merged_df['p']==p][col])\n",
    "                auprc_list.append(score)\n",
    "            auprc_dict[col] = auprc_list\n",
    "   \n",
    "\n",
    "        # Plot the precision columns on the third subplot\n",
    "        auroc_dict = {}\n",
    "        for col in auroc_cols:\n",
    "            matching_methods = [method for method in methods if method in col]\n",
    "            color = [color_mapping[method] for method in matching_methods][0]\n",
    "            p_list = merged_df['p'].unique()\n",
    "            auroc_list = []\n",
    "            for p in p_list:\n",
    "                score = np.mean(merged_df[merged_df['p']==p][col])\n",
    "                if score<0.5:\n",
    "                    score=1-score\n",
    "                auroc_list.append(score)\n",
    "            auroc_dict[col] = auroc_list\n",
    "  \n",
    "\n",
    "        # Call plot_rank for each subplot\n",
    "        data_prc, perf_prc = data_and_perf_dicts(auprc_dict,  prop_list=prop_list)\n",
    "        data_roc, perf_roc = data_and_perf_dicts(auroc_dict, prop_list=prop_list)\n",
    "\n",
    "        \n",
    "        myproject = f\"update_{hardness}\"\n",
    "\n",
    "\n",
    "\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        #\n",
    "        # HEATMAP plot (PRC)\n",
    "        #\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        dictionary = perf_prc\n",
    "\n",
    "        # Convert the dictionary to a 2D NumPy array\n",
    "        data = np.array([[value for value in level.values()] for level in dictionary.values()])\n",
    "\n",
    "        # Create a list of keys and levels for labeling the heatmap\n",
    "        keys = list(dictionary.values())[0].keys()\n",
    "        keys = [key.split(\".\")[0] for key in keys]\n",
    "        levels = list(dictionary.keys())\n",
    "\n",
    "        # Create a list of tuples (group, method) for each key\n",
    "        grouped_keys = [(groups.get(key, \"Dist\"), key) for key in keys]\n",
    "\n",
    "        # Sort the keys and data by group\n",
    "        sorted_grouped_keys = sorted(grouped_keys, key=lambda x: (x[0], keys.index(x[1])))\n",
    "\n",
    "        # Extract the sorted keys and group labels\n",
    "        sorted_keys = [key[1] for key in sorted_grouped_keys]\n",
    "        group_labels = [key[0] for key in sorted_grouped_keys]\n",
    "\n",
    "        # Create an ordered dictionary that maps the original keys to their corresponding data columns\n",
    "        data_dict = collections.OrderedDict(zip(keys, data.T))\n",
    "\n",
    "        # Create a new sorted dictionary and a new sorted data array\n",
    "        sorted_data_dict = collections.OrderedDict(sorted(data_dict.items(), key=lambda x: sorted_grouped_keys.index((groups.get(x[0], \"Dist\"), x[0]))))\n",
    "\n",
    "        # Get the indices of the sorted keys\n",
    "        indices = [keys.index(key) for key in sorted_keys]\n",
    "\n",
    "        # Reorder the data array based on the sorted keys\n",
    "        sorted_data = data[:, indices]\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(4, 2))\n",
    "        heatmap = ax.imshow(sorted_data, cmap='bwr_r', vmin=np.round(np.min(sorted_data),1), vmax=np.round(np.max(sorted_data),1))\n",
    "\n",
    "        # Remove grid\n",
    "        ax.grid(False)\n",
    "\n",
    "        # Add custom gridlines\n",
    "        rows, cols = sorted_data.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                if i != rows - 1:\n",
    "                    ax.axhline(i + 0.5, color='white', linewidth=0.5)\n",
    "                if j != cols - 1:\n",
    "                    ax.axvline(j + 0.5, color='white', linewidth=0.5)\n",
    "\n",
    "        # Remove grid\n",
    "        ax.grid(False)\n",
    "\n",
    "        fs=8\n",
    "\n",
    "        # Set the ticks and labels for the x and y axes\n",
    "        ax.set_xticks(np.arange(len(keys)))\n",
    "        ax.set_yticks(np.arange(len(levels)))\n",
    "        ax.set_yticklabels(prop_list, fontsize=fs) #old levels\n",
    "\n",
    "        plt.minorticks_off()\n",
    "        plt.tick_params(axis='both', which='both', bottom=True, top=False, labelbottom=True, left=True, right=False, labelleft=True, direction='out')\n",
    "        plt.xticks(fontsize=fs)\n",
    "        plt.yticks(fontsize=fs)\n",
    "\n",
    "        rawlabels = list(sorted_data_dict.keys())\n",
    "\n",
    "        labels = []\n",
    "        for label in rawlabels:\n",
    "\n",
    "            if label =='prototypicality':\n",
    "                labels.append('proto')\n",
    "            elif label =='forgetting':\n",
    "                labels.append('forget')\n",
    "            elif label =='conf_agree':\n",
    "                labels.append('conf_agr')\n",
    "            else:\n",
    "                labels.append(label)\n",
    "\n",
    "        # Rotate the method names on the x-axis for better readability\n",
    "        ax.set_xticklabels(labels, rotation=45, ha=\"right\", rotation_mode=\"anchor\", fontsize=fs)\n",
    "        ax.tick_params(axis='x', pad=-1)\n",
    "\n",
    "        group_positions = {}\n",
    "        for i, key in enumerate(sorted_data_dict.keys()):\n",
    "            group = groups.get(key, \"Dist\")\n",
    "            if group not in group_positions:\n",
    "                group_positions[group] = [i]\n",
    "            else:\n",
    "                group_positions[group].append(i)\n",
    "\n",
    "        # Now, for each group, find the midpoint and add the label there\n",
    "        for group, positions in group_positions.items():\n",
    "            midpoint = np.mean(positions)\n",
    "            ax.text(midpoint, -0.55, group, ha='center', va='top', transform=ax.get_xaxis_transform(), fontsize=6)\n",
    "\n",
    "\n",
    "            # # Draw vertical lines at the boundaries of each group\n",
    "            if len(positions) > 1:\n",
    "                ax.axvline(x=positions[0]-0.5,  color='k', linestyle='--',)\n",
    "                ax.axvline(x=positions[-1]+0.5, color='k', linestyle='--')\n",
    "\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = ax.figure.colorbar(heatmap, shrink=0.55)\n",
    "        cbar.ax.tick_params(labelsize=fs) \n",
    "\n",
    "        # Set the title and labels\n",
    "\n",
    "        ax.set_ylabel(\"Proportion perturbed\", fontsize=fs)\n",
    "        title = f\"{hardness}_{model}_{dataset_name}\"\n",
    "        #plt.title(title, fontsize=fs)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplots_adjust(bottom=0)\n",
    "\n",
    "        plt.savefig(f'{folder}/heatmaps_{myproject}_prc.pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        #\n",
    "        #  Rank violin (PRC)\n",
    "        #\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "\n",
    "        import pandas as pd\n",
    "        import seaborn as sns\n",
    "\n",
    "        rank_scores = data_prc\n",
    "\n",
    "        # Convert the dictionary to a DataFrame for easier manipulation\n",
    "        df = pd.DataFrame.from_dict(rank_scores, orient='index')\n",
    "        labels = []\n",
    "        rawlabels = [name.split(\".\")[0] for name in list(df.columns)]\n",
    "        for label in rawlabels:\n",
    "\n",
    "            if label =='prototypicality':\n",
    "                labels.append('proto')\n",
    "            elif label =='forgetting':\n",
    "                labels.append('forget')\n",
    "            elif label =='conf_agree':\n",
    "                labels.append('conf_agr')\n",
    "            else:\n",
    "                labels.append(label)\n",
    "        df.columns = labels\n",
    "        method_order = sorted(df.columns)\n",
    "        df = df.reindex(sorted(df.columns), axis=1)\n",
    "        # Set the figure size\n",
    "        plt.figure(figsize=(6, 2))\n",
    "\n",
    "        # Create the violin plot without scaling the width\n",
    "        sns.violinplot(data=df, inner=\"stick\", scale=\"width\",cut=0, order=method_order)\n",
    "\n",
    "        # Add median values to the violin plot\n",
    "        medians = df.median()\n",
    "        vertical_offset = df.values.max() * 0.01  # adjust this value for better alignment\n",
    "        for xtick in range(df.shape[1]):\n",
    "            plt.text(xtick, medians[xtick] + vertical_offset, round(medians[xtick],2),\n",
    "                    horizontalalignment='center', color='black')\n",
    "\n",
    "        # Set labels and title\n",
    "        #plt.xlabel('HCM')\n",
    "        plt.ylabel('Rank Scores', fontsize=14)\n",
    "\n",
    "        # \n",
    "        # Rotate x-axis labels if needed\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tick_params(axis='x', pad=-5)\n",
    "\n",
    "        # Flip the y-axis\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "        plt.minorticks_off()\n",
    "        plt.tick_params(axis='both', which='both', bottom=True, top=False, labelbottom=True, left=True, right=False, labelleft=True, direction='out')\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=16)\n",
    "        plt.yticks(range(1, int(df.values.max()) + 1,2))\n",
    "\n",
    "        #plt.title(f\"{hardness}_prc\")\n",
    "        plt.savefig(f'{folder}/violin_{myproject}_prc.pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # Initialize an empty matrix to store the counts\n",
    "        rank_scores = data_prc\n",
    "\n",
    "        # Convert the dictionary to a DataFrame for easier manipulation\n",
    "        df = pd.DataFrame.from_dict(rank_scores, orient='index')\n",
    "        labels = []\n",
    "        rawlabels = [name.split(\".\")[0] for name in list(df.columns)]\n",
    "        for label in rawlabels:\n",
    "\n",
    "            if label =='prototypicality':\n",
    "                labels.append('proto')\n",
    "            elif label =='forgetting':\n",
    "                labels.append('forget')\n",
    "            elif label =='conf_agree':\n",
    "                labels.append('conf_agr')\n",
    "            else:\n",
    "                labels.append(label)\n",
    "        df.columns = labels\n",
    "        data=df\n",
    "        data = data.sort_index(axis=0).sort_index(axis=1)\n",
    "        matrix = pd.DataFrame(0, index=data.columns, columns=data.columns)\n",
    "\n",
    "        # Iterate over the columns to calculate the counts\n",
    "        for col1 in data.columns:\n",
    "            for col2 in data.columns:\n",
    "                matrix.loc[col1, col2] = sum(data[col1] < data[col2])  # Count how many times col1 < col2\n",
    "\n",
    "\n",
    "        # Normalize the matrix between 0 and 1\n",
    "        matrix_normalized = matrix / matrix.max().max()\n",
    "\n",
    "        # Replace diagonal elements with 'X'\n",
    "        np.fill_diagonal(matrix_normalized.values, 0.5)\n",
    "\n",
    "\n",
    "        # Plot the normalized heatmap matrix with gridlines\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(matrix_normalized, cmap='bwr_r', fmt='.1f', linewidths=0.5, linecolor='black')\n",
    "        #plt.title('Method Beats Matrix (Normalized)')\n",
    "        plt.xlabel('HCM 2')\n",
    "        plt.ylabel('HCM 1')\n",
    "        plt.savefig(f'{folder}/mat_compare_{myproject}_prc.pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        #\n",
    "        # CD PLOT (PRC)\n",
    "        #\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "\n",
    "        data = data_prc\n",
    "\n",
    "        pdf = pd.DataFrame(data)\n",
    "        pdf.index = [name.split(\".\")[0] for name in pdf.index]\n",
    "\n",
    "        # reset index to treat method names as a column\n",
    "        pdf = pdf.reset_index()\n",
    "\n",
    "        # melt the DataFrame to long format\n",
    "        df_melted = pd.melt(pdf, id_vars='index', var_name='score_key', value_name='rank')\n",
    "\n",
    "        # # rename 'index' column to 'method'\n",
    "        df_melted.rename(columns={'index': 'method', \"rank\": \"score\"}, inplace=True)\n",
    "        avg_rank = df_melted.groupby('score_key').score.rank(pct=False, ascending=True).groupby(df_melted.method).mean()\n",
    "\n",
    "        import scikit_posthocs as sp\n",
    "        from src.plots import *\n",
    "\n",
    "        test_results = sp.posthoc_siegel_friedman(\n",
    "        df_melted,\n",
    "        melted=True,\n",
    "        block_col='score_key',\n",
    "        group_col='method',\n",
    "        y_col='score',\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(3.5, 2), dpi=1000)\n",
    "        #plt.title(f\"{hardness}_prc\")\n",
    "        critical_difference_diagram(avg_rank, test_results, label_props={'color': 'black', 'fontsize': 12},text_h_margin=0.1,)\n",
    "        plt.savefig(f'{folder}/cd_{myproject}_prc1.pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        print(f'Failed - {myproject}')\n",
    "        failed.append(myproject)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardness_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a7e60037fa700cd8eaefe68718883a7c5484a19cbbdd784476a6a2fe63bc7d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
